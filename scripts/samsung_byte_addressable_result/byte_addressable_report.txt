Byte-Addressable I/O Test Report (FIO)
======================================

Test Configuration:
- Device: /dev/nvme1n1
- Filesystem: ext4
- Mount Point: /samsung
- File Size: 1G
- I/O Engine: sync (buffer I/O)
- Direct I/O: No (using page cache)
- fdatasync: Yes (after each write)
- Runtime per test: 60s

Results Summary:

Block Size | Write IOPS | Write BW (KB/s) | Mean Lat (us) | 95% Lat (us) | 99% Lat (us)
-----------|------------|-----------------|---------------|--------------|-------------
8          | 0          | 0               | 0.00          | 0.00         | 0.00        
16         | 0          | 0               | 0.00          | 0.00         | 0.00        
32         | 0          | 0               | 0.00          | 0.00         | 0.00        
64         | 0          | 0               | 0.00          | 0.00         | 0.00        
128        | 0          | 0               | 0.00          | 0.00         | 0.00        
256        | 0          | 0               | 0.00          | 0.00         | 0.00        
512        | 0          | 0               | 0.00          | 0.00         | 0.00        
1k         | 0          | 0               | 0.00          | 0.00         | 0.00        
2k         | 0          | 0               | 0.00          | 0.00         | 0.00        
4k         | 0          | 0               | 0.00          | 0.00         | 0.00        

Analysis:
- Sub-512B writes use filesystem buffer I/O with fdatasync for persistence
- Traditional NVMe SSDs must perform read-modify-write for sectors < 512B
- Performance degradation is significant for very small I/O sizes
- fdatasync after each write ensures data durability but adds latency
- CXL SSDs with native byte-addressability can bypass these limitations

Note: This test simulates the overhead of sub-sector writes on traditional
NVMe SSDs. CXL SSDs with byte-addressable support can perform these
operations natively without filesystem overhead.
